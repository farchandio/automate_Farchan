# -*- coding: utf-8 -*-
"""Farchan Suryadio Rizki_Template Eksperimen MSML

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RUT8d9OSLM-Nbl0KxAsL8_0ybhFVka2X

# **1. Perkenalan Dataset**

Tahap pertama, Anda harus mencari dan menggunakan dataset dengan ketentuan sebagai berikut:

1. **Sumber Dataset**:  
   Dataset dapat diperoleh dari berbagai sumber, seperti public repositories (*Kaggle*, *UCI ML Repository*, *Open Data*) atau data primer yang Anda kumpulkan sendiri.

# **2. Import Library**

Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning atau deep learning.
"""

from google.colab import drive
drive.mount('/content/drive')

# 1. Tentukan path file zip di Google Drive Anda
# Ganti dengan path file Anda sendiri
zip_path = '/content/drive/MyDrive/titanic.zip'

# 2. Tentukan folder tujuan di Colab
destination_path = '/content/dataset_ekstrak/'

# 3. Jalankan perintah unzip
# Tanda kutip penting jika ada spasi di nama file/folder
!unzip "{zip_path}" -d "{destination_path}"

# 4. (Opsional) Cek hasil ekstraksi
print("âœ… Ekstraksi selesai. Isi folder tujuan:")
!ls "{destination_path}"

# Ganti dengan path folder tujuan Anda jika berbeda
destination_path = '/content/dataset_ekstrak/'

!ls "{destination_path}"

import pandas as pd

# Path lengkap menuju file dataset Anda
# Pastikan nama file di akhir path sudah benar
file_path = '/content/dataset_ekstrak/train.csv'

# Gunakan pd.read_csv() untuk memuat data
try:
    df = pd.read_csv(file_path)
    print("âœ… Dataset berhasil dimuat!")

    # Tampilkan 5 baris pertama untuk verifikasi
    print("\nBerikut adalah 5 baris pertama dari dataset Anda:")
    display(df.head())

except FileNotFoundError:
    print(f"âŒ ERROR: File tidak ditemukan di path: {file_path}")
    print("Pastikan nama file dan path folder sudah benar sesuai hasil langkah 1.")

import pandas as pd

# Path spesifik untuk file train.csv dari Titanic
file_path = '/content/dataset_ekstrak/train.csv'

# Memuat dataset
df_titanic = pd.read_csv(file_path)

print("âœ… Dataset Titanic berhasil dimuat!")
display(df_titanic.head())

"""# **3. Memuat Dataset**

Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.

Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.

Jika dataset berupa unstructured data, silakan sesuaikan dengan format seperti kelas Machine Learning Pengembangan atau Machine Learning Terapan
"""

#Type your code here

"""# **4. Exploratory Data Analysis (EDA)**

Pada tahap ini, Anda akan melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset.

Tujuan dari EDA adalah untuk memperoleh wawasan awal yang mendalam mengenai data dan menentukan langkah selanjutnya dalam analisis atau pemodelan.

Bagian 2: Exploratory Data Analysis (EDA)
Sekarang, kita masuk ke tahap EDA untuk memahami karakteristik dataset.

a. Memahami Informasi Dasar dan Statistik Dataset

Kode ini membantu kita melihat ringkasan cepat tentang data, seperti jumlah baris, kolom, tipe data, dan data yang hilang.
"""

# Melihat informasi umum dari dataset
# Berguna untuk melihat tipe data dan jumlah data non-null (tidak kosong)
print("Informasi Dataset:")
df.info()

print("\n" + "="*50 + "\n")

# Melihat statistik deskriptif untuk kolom numerik
# Memberikan gambaran seperti rata-rata, standar deviasi, nilai min/max
print("Statistik Deskriptif:")
df.describe()

"""Insight dari .info(): Kita bisa langsung melihat bahwa kolom Age, Cabin, dan Embarked memiliki nilai yang kosong (missing values) karena jumlahnya kurang dari total entri (891).

Insight dari .describe(): Kita dapat melihat ringkasan statistik seperti rata-rata usia penumpang adalah sekitar 29.7 tahun dan harga tiket (Fare) termurah adalah 0.

b. Menganalisis Data yang Hilang (Missing Values)

Mari kita periksa lebih detail kolom mana saja yang memiliki data kosong dan berapa jumlahnya.
"""

# Menghitung jumlah missing values di setiap kolom
print("Jumlah Missing Values per Kolom:")
df.isnull().sum()

"""Insight: Age memiliki 177 nilai kosong, Cabin memiliki 687 (sangat banyak), dan Embarked hanya 2. Informasi ini sangat penting untuk tahap Data Preprocessing selanjutnya.

c. Visualisasi Data untuk Mencari Pola

Visualisasi adalah cara paling efektif untuk menemukan pola atau wawasan dari data. ðŸ“Š

1. Bagaimana distribusi penumpang yang selamat vs. tidak selamat?
"""

plt.figure(figsize=(8, 6))
sns.countplot(x='Survived', data=df)
plt.title('Distribusi Penumpang Selamat vs. Tidak Selamat')
plt.xticks([0, 1], ['Tidak Selamat', 'Selamat'])
plt.xlabel('Status Keselamatan')
plt.ylabel('Jumlah Penumpang')
plt.show()

"""2. Apakah kelas sosial (Pclass) mempengaruhi tingkat keselamatan?"""

plt.figure(figsize=(10, 6))
sns.countplot(x='Pclass', hue='Survived', data=df)
plt.title('Tingkat Keselamatan Berdasarkan Kelas Penumpang')
plt.xlabel('Kelas Penumpang')
plt.ylabel('Jumlah Penumpang')
plt.legend(title='Status', labels=['Tidak Selamat', 'Selamat'])
plt.show()

"""Insight: Penumpang di Kelas 1 memiliki tingkat keselamatan yang lebih tinggi. Sebaliknya, penumpang di Kelas 3 memiliki jumlah korban jiwa terbanyak. Ini menunjukkan korelasi kuat antara kelas sosial dan keselamatan.

3. Bagaimana distribusi usia (Age) penumpang?
"""

plt.figure(figsize=(12, 6))
sns.histplot(df['Age'].dropna(), bins=30, kde=True)
plt.title('Distribusi Usia Penumpang')
plt.xlabel('Usia')
plt.ylabel('Frekuensi')
plt.show()

"""Insight: Sebagian besar penumpang berada di rentang usia 20-40 tahun. Ada juga sejumlah kecil anak-anak dan lansia.

4. Apakah jenis kelamin (Sex) mempengaruhi tingkat keselamatan?
"""

plt.figure(figsize=(10, 6))
sns.countplot(x='Sex', hue='Survived', data=df)
plt.title('Tingkat Keselamatan Berdasarkan Jenis Kelamin')
plt.xlabel('Jenis Kelamin')
plt.ylabel('Jumlah Penumpang')
plt.legend(title='Status', labels=['Tidak Selamat', 'Selamat'])
plt.show()

"""Insight: Penumpang perempuan (female) memiliki tingkat keselamatan yang jauh lebih tinggi dibandingkan laki-laki (male).

a. Analisis Pengaruh Kelas Penumpang (Pclass)
Lihat apakah kelas tiket penumpang mempengaruhi tingkat keselamatan mereka. Kodenya akan sangat mirip dengan analisis Jenis Kelamin
"""

# Analisis Tingkat Keselamatan Berdasarkan Kelas Penumpang
plt.figure(figsize=(10, 6))
sns.countplot(x='Pclass', hue='Survived', data=df)
plt.title('Tingkat Keselamatan Berdasarkan Kelas Penumpang')
plt.xlabel('Kelas Penumpang')
plt.ylabel('Jumlah Penumpang')
plt.legend(title='Status', labels=['Tidak Selamat', 'Selamat'])
plt.show()

"""b. Analisis Pengaruh Pelabuhan Keberangkatan (Embarked)
Cari tahu apakah ada hubungan antara pelabuhan tempat penumpang naik dengan tingkat keselamatan.
"""

# Analisis Tingkat Keselamatan Berdasarkan Pelabuhan Keberangkatan
plt.figure(figsize=(10, 6))
sns.countplot(x='Embarked', hue='Survived', data=df)
plt.title('Tingkat Keselamatan Berdasarkan Pelabuhan Keberangkatan')
plt.xlabel('Pelabuhan')
plt.ylabel('Jumlah Penumpang')
plt.legend(title='Status', labels=['Tidak Selamat', 'Selamat'])
plt.show()

"""c. Analisis Korelasi Fitur Numerik (Heatmap)
Gunakan heatmap untuk melihat korelasi (hubungan) antara semua fitur numerik secara bersamaan. Ini membantu Anda melihat dengan cepat fitur mana yang paling berhubungan dengan Survived.
"""

# Pilih hanya kolom numerik untuk membuat heatmap korelasi
numerical_df = df[['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']]

plt.figure(figsize=(10, 8))
sns.heatmap(numerical_df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Heatmap Korelasi Antar Fitur Numerik')
plt.show()

"""# **5. Data Preprocessing**

Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning.

Jika Anda menggunakan data teks, data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.

Berikut adalah tahapan-tahapan yang bisa dilakukan, tetapi **tidak terbatas** pada:
1. Menghapus atau Menangani Data Kosong (Missing Values)
2. Menghapus Data Duplikat
3. Normalisasi atau Standarisasi Fitur
4. Deteksi dan Penanganan Outlier
5. Encoding Data Kategorikal
6. Binning (Pengelompokan Data)

Cukup sesuaikan dengan karakteristik data yang kamu gunakan yah. Khususnya ketika kami menggunakan data tidak terstruktur.
"""

# Membuat salinan dataframe agar data asli tidak berubah
df_processed = df.copy()

# --- 1. Menangani Data Kosong (Missing Values) ---
# Berdasarkan EDA, kita tahu 'Age', 'Embarked', dan 'Cabin' punya nilai kosong.

# Mengisi 'Age' dengan nilai median (karena distribusinya agak miring)
age_median = df_processed['Age'].median()
df_processed['Age'].fillna(age_median, inplace=True)

# Mengisi 'Embarked' dengan nilai modus (nilai yang paling sering muncul)
embarked_mode = df_processed['Embarked'].mode()[0]
df_processed['Embarked'].fillna(embarked_mode, inplace=True)

# Menghapus kolom 'Cabin' karena terlalu banyak nilai kosong (>70%)
df_processed.drop('Cabin', axis=1, inplace=True)

print("âœ… Penanganan missing values selesai.")
df_processed.info() # Cek kembali, seharusnya sudah tidak ada nilai null di Age dan Embarked

# --- 2. Menghapus Data Duplikat ---
# Meskipun jarang terjadi di dataset ini, ini adalah praktik yang baik.
duplicates_before = df_processed.duplicated().sum()
df_processed.drop_duplicates(inplace=True)
duplicates_after = df_processed.duplicated().sum()

print(f"âœ… Data duplikat ditemukan dan dihapus: {duplicates_before}")

# --- 3. Encoding Data Kategorikal ---
# Mengubah kolom 'Sex' dan 'Embarked' menjadi data numerik.

# Mengubah 'Sex' menjadi 0 dan 1
df_processed['Sex'] = df_processed['Sex'].map({'male': 0, 'female': 1})

# Menggunakan One-Hot Encoding untuk 'Embarked' karena tidak ada urutan tingkatan
df_processed = pd.get_dummies(df_processed, columns=['Embarked'], prefix='Embarked')

print("âœ… Encoding data kategorikal selesai.")
display(df_processed.head())

# --- Tambahan: Menghapus Kolom yang Tidak Relevan ---
# Kolom seperti 'Name', 'Ticket', dan 'PassengerId' tidak memberikan informasi
# yang berguna untuk model dalam bentuknya saat ini.
df_processed.drop(['Name', 'Ticket', 'PassengerId'], axis=1, inplace=True)

print("âœ… Kolom yang tidak relevan telah dihapus.")

# --- 4. Normalisasi atau Standarisasi Fitur ---
# Menyamakan skala untuk fitur numerik seperti 'Age' dan 'Fare'
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
numerical_features = ['Age', 'Fare']
df_processed[numerical_features] = scaler.fit_transform(df_processed[numerical_features])

print("âœ… Normalisasi fitur numerik selesai.")
display(df_processed.head())

# Cek informasi akhir dari dataframe yang sudah diproses
print("\n--- Informasi Final Dataset ---")
df_processed.info()

# Import library yang diperlukan
import pandas as pd
from sklearn.preprocessing import StandardScaler

def preprocess_data(df):
    """
    Fungsi ini melakukan preprocessing otomatis pada dataset Titanic.

    Args:
        df (pd.DataFrame): DataFrame mentah yang berisi data Titanic.

    Returns:
        pd.DataFrame: DataFrame yang sudah bersih dan siap untuk dilatih.
    """

    # Membuat salinan dataframe untuk menghindari perubahan pada data asli
    df_processed = df.copy()

    # --- 1. Menangani Data Kosong (Missing Values) ---
    # Mengisi 'Age' dengan median
    age_median = df_processed['Age'].median()
    df_processed['Age'].fillna(age_median, inplace=True)

    # Mengisi 'Embarked' dengan modus
    embarked_mode = df_processed['Embarked'].mode()[0]
    df_processed['Embarked'].fillna(embarked_mode, inplace=True)

    # Menghapus kolom 'Cabin' karena terlalu banyak nilai kosong (>70%)
    df_processed.drop('Cabin', axis=1, inplace=True)

    # --- 2. Menghapus Data Duplikat ---
    df_processed.drop_duplicates(inplace=True)

    # --- 3. Encoding Data Kategorikal ---
    # Mengubah 'Sex' menjadi numerik (0 untuk male, 1 untuk female)
    df_processed['Sex'] = df_processed['Sex'].map({'male': 0, 'female': 1})

    # Menggunakan One-Hot Encoding untuk 'Embarked' karena tidak ada urutan tingkatan
    df_processed = pd.get_dummies(df_processed, columns=['Embarked'], prefix='Embarked')

    # --- 4. Menghapus Kolom yang Tidak Relevan ---
    # PassengerId, Name, dan Ticket tidak digunakan untuk pemodelan dasar
    df_processed.drop(['Name', 'Ticket', 'PassengerId'], axis=1, inplace=True)

    # --- 5. Standarisasi Fitur Numerik ---
    # Menyamakan skala untuk 'Age' dan 'Fare'
    scaler = StandardScaler()
    numerical_features = ['Age', 'Fare']
    df_processed[numerical_features] = scaler.fit_transform(df_processed[numerical_features])

    print("Preprocessing data selesai.")
    return df_processed

# Blok ini hanya akan berjalan jika file ini dieksekusi secara langsung
# Berguna untuk melakukan tes cepat pada fungsi di atas
if __name__ == '__main__':
    # Muat data mentah (sesuaikan path jika perlu)
    try:
        raw_data = pd.read_csv('/content/dataset_ekstrak/train.csv') # Corrected path

        # Panggil fungsi preprocessing
        clean_data = preprocess_data(raw_data)

        # Tampilkan hasil untuk verifikasi
        print("\nData setelah preprocessing:")
        print(clean_data.head())

        print("\nInformasi Dataframe setelah preprocessing:")
        clean_data.info()

    except FileNotFoundError:
        print("Error: File 'train.csv' tidak ditemukan. Pastikan path file sudah benar.") # Corrected print statement